# statistics_09

# 회귀분석 고급

## 완전 분리 complete separation

- 두 집단이 100% 완전하게 분리될 수 있는 경우
- 로지스틱 회귀분석에서 계수가 무한대로 발산
- 로지스틱 곡선은 0과 1로 접근하므로 1(=100%)에 가까이 가려면 무한대의 계수가 필요
- 완전 분리가 가능한 경우는 로지스틱 회귀분석을 쓰지 않음

## 혼돈행렬

## 혼돈행렬을 이용한 주요 지표

- 정확도(accuracy): 전체 중 몇 %를 맞았나?
- 정밀도(precision, PPV): +로 예측한 것 중에 몇 %를 맞았나?
    - 분모가 예측
    - 신입사원을 뽑을 때는 '일을 잘할거야'라고 예측해서 뽑은 신입사원 중 몇 %가 진짜냐
    - 들여보낸 사람 중에 좀비와 사람이 몇 %냐
    - 합격/ 합격+예측  : 입시컨설턴트 입장에서 고등학생들이 대학교에 지원해서 몇 %합격했는 지를 예측했을 때, 실제 합격한 %
    - 주식이 오른 거/산 주식
    - 잘된 신상품/ 새로운 신상품 런칭
    - 대학생 입장: a학점/a학점 예측
    - 소비자 입장 : 너구리 다시마가 있음/ 다시마 예측(있을 거라고)
- 재현도(recall, TPR): 실제로 +인 것 중에 몇 %를 맞았나?
    - 꼭 세상의 모든? - 아니 범위 정해주면 됨 (내가 다루는 데이터의 범위 내에서만)
    - 분모가 실제
    - 전염병이 돌 때, 전염병이 걸렸다고 예측한 돼지가 전체에서 몇 %인가
    - 밖에 있는 사람들 중에 좀비와 사람이 몇 %냐
    - 산 주식/ 주식이 오른 거
    - 잡힌 성범죄자/ 세상의 성범죄자
    - ~~예측한 a학점/a 학점 ? 이게 중요한가~~
    - 검사해서 골라 냄/불량품 → 그치만 recall 100% 면 의미가 없는 데이터임(정상도 다 불량처리하면 됨) → 그니까 reall과 precision을 균형있게 봐야 한다.
- F1: 정밀도와 재현도의 조화 평균

## ROC Curve

- 가로축은 FPR(정밀도), 세로축은 TPR(재현도)
- 동일한 분류 방법에서 양성(P) 예측을 많이 하면 TPR과 FPR이 오름
- 틀린 예측(FPR)에 비해 옳은 예측(TPR)이 얼마나 빨리 오르는가?
- ROC 곡선이 왼쪽 위로 많이 휠 수록 좋은 분류 방법
- 공장 품질 관리 예 P : 불량
- fpr = 불량처리 / 멀쩡한 제품
- threshold 기준값 예시
    - 로지스틱 -(예측)→ 확률
        - positive와 negative의 경계를 정함. 몇 %일 때 번아웃인지 정할 것인가, 통상적으로 많이 쓰는 것은 50%
        - 병 진단 할 때, threshold를 낮으면 조금만 병이 있어도 추가 검사를 하게 됨
        - threshold를 낮으면 높으면 큰 병이 있어도 추가 검사를 안하게 됨 (오진률 올라감)

## ROC 곡선 아래 면적

- 무작위로 예측할 때 0.5 (여기보다 낮으면 찍는 것보다 못하는 경우 : 쓰레기 모델)
- 완벽하게 예측할 때 1

## 실제

- 데이터가 2 세트가 있어야 함.
- 성능지표를 계산하려면 정답은 알아야 하니까 하나는 아는 거, 한 데이터는 모르는 거를 해서 모르는 것을 예측해서 정답을 아는 데이터로 비교할 수 있음

## 선형 모형이 아닌 다른 예측 방법들

- 최근접 이웃
- 의사결정나무
- SVM
- 가우시안 프로세스
- 인공신경망(딥러닝)
- 등등

## 선형 모형을 많이 쓰는 이유

- 모형이 단순
- 과적합이 적다
- 해석이 쉽다

## 최근접 이유 Nearest Neighbor

- 예측하고자 하는 사례와 가장 비슷한 사례를 k개 찾아 동일하게 예측
- 유사도(similarity)를 정할 수 있어야 함
- 좋은 방법인데 단점,
- 데이터가 많아지면 유사 사례를 찾는 시간이 오래 걸림
- 데이터가 적으면 많은 사례 수(k)를 찾을 수록 예측력이 떨어짐
    - 비슷한 정도를 수치화 할 수 있어야 함 (그런데 어려운 경우가 많음)
    - 유사도에 따라 결과가 많이 달라짐
    - k=3 이다고 할 때, 유사한 3개를 찾는다 (아파트 값 찾고 싶을 때, 비슷한 아파트 가격 3개 찾기)

## 의사결정나무 Decision Tree

- 스무고개와 같이 Y/N 질문을 반복하여 예측
- 질문의 수가 적으면 해석하기가 쉬움
- 데이터가 조금만 달라져도 결과가 크게 달라질 수 있음

## 앙상블 Ensemble

- 하나의 모형으로 예측을 잘 하는데는 한계
- 앙상블: 여러 개의 모형을 만들어 평균/다수결로 예측하는 방법
- 요즘 많이 씀 (넷플릭스)
- 최근 머신러닝은 그래디언트부스팅, 딥러닝 등 앙상블이 주도

# 로지스틱 회귀분석과 클러스터링

## 클러스터링 clustering

- 비슷한 사례들끼리 군집(cluster)로 모으는 것
- 대부분 군집의 수는 하이퍼파라미터로 정해주어야 함
    - 예시 : 신용카드의 고객 유형 분류

## 분류와 클러스터링의 차이

- 로지스틱 회귀분석과 같은 분류에서는 각 사례의 범주가 관찰변수
- 최소한 파라미터 추정 단계에서는 범주가 알려져 있음
- 예측할 때는 모를 수도 있음
- 클러스터링에서는 범주가 잠재변수
- 분류 : 관찰 범주 (뭐다 아니다로 나눌 수 있음 (정답 존재))
- 클러스터링 : 범주이긴 하는데 관찰이 안 됨. 정답이 존재하지 않고 나눌 수 없음

## 클러스터링의 종류

- **K-means**
- Affinity Propagation
- Mean Shift
- Spectral clustering
- **Hierarchical clustering**
- **DBSCAN**
- OPTICS
- Birch
- 같은 색깔이면 같은 클러스터로 나누어진 거
- (어떤 기법은 거리를 중시하고, 어떤 기법은 서로 이어져 있는 것을 중시함)

![](Untitled-e2139083-a0c6-4a80-bc0e-9ecb36c2d04c.png)

## K-means

- 가장 널리 사용되는 클러스터링 방법
- K-means는 k개의 평균이라는 뜻 (k 집단으로 나눔)
- 군집의 평균(중심점)을 구함
- 사례를 중심점이 가장 가까운 군집에 포함시킴
- 비슷한 집단끼리는 평균 근처에 몰려있을 것이다 (그 평균)
    - a 집단 → a 평균/ b집단 → b평균/ c집단→ c평균
- wine 데이터로 실제 작동해보자! (주피터 노트북 day9)

![](Untitled-2ca1ac0b-63b4-4c08-af6c-a4519e95e756.png)

- 대략 평균 같은 곳에 점 2(k)개를 찍고 분류를 해서 나중에는 최적의 조건을 찾음

![](Untitled-7427942a-4eb9-4794-8f84-93b55e83d4ad.png)

## k-Means의 특징

- k만 정해주면 되므로 간단
- 단점 1: 거리를 정할 수 있고, 중심점 주변에 사례들이 몰려있는 경우에 사용 (안 맞으면 사용하기 힘듬)
- 단점2: 분류를 계속하고 평균을 구하고, 분류를 계속 하고 평균을 구하고를 반복하기 때문에 데이터가 많을 수록 시간이 오래걸림
- 데이터가 많을 때는, 소수의 사례만 무작위로 뽑아 클러스터링할 수도 있음(미니배치 kMeans)

## Affinity Propagation (직역 : 친밀감 전파)

- 중심점 대신 대표(exemplar)를 찾음 - 평균점 대신 대표를 찾음!
    - exemplar의 용법: 새의 exemplar는 참새
- responsibility와 availability라는 수치를 반복적으로 계산
- 각 사례의 대표가 바뀌지 않을 때(수렴)까지 반복
- 클러스터의 개수를 미리 정하지 않음 (안 정해줘도 됨 대신 preference 지정해줌)
- 데이터가 적을 때 사용

- 가까이 있는 군집을 찾음 → 대표를 찾음 → 각 사례의 대표가 바뀌지 않을 때까지 반복

![](Untitled-efafe11f-dfc4-4136-af3c-4c2e6656b2ca.png)

## Mean Shift

- 각 사례의 중심점을 수렴할 때까지 이동
- 맨 처음 중심점 - 자기자신
- 중심점 주변의 사례를 모음 → 평균 → 새로운 중심점 (반지름 중심)
- 클러스터의 개수를 미리 정하지 않음 (k 안정해도 됨, 대신 반지름을 구해야함)
- 데이터가 적을 때 사용

![](Untitled-3b44278d-ac16-436b-81d8-4def199a3e3a.png)

## Spectral Clustering

- 비슷한 사례들끼리 연결하여 그래프(graph)를 만듦 (점들을 선으로 이음)
- 이 때, 비슷한 점들끼리 연결 된 선을 끊으면 어디가 잘 뭉쳐질까?
- 장점 : k-means는 평균 주변에 뭉쳐져 있어야 하는데, 이 방식은 덜 뭉쳐있어도 잘 작동함
    - 절대적인 거리가 중요한 게 아니라, 서로서로 얼마나 근접한 이웃인가
    - 엄밀하게 거리가 정해져있지 않을 때 쓰면 좋음
    - (내 친구의 친구는 나와 거리가 멀수도 있지만 친구의 친구니까 한그루! 이런 아이디어)
    - 페이스북 사용자 클러스터링
- 그래프의 구조를 반영하여 클러스터링
- 클러스터의 수를 미리 정해줘야 함
- 데이터가 적을 때 사용

## 위계적 클러스터링 Hierarchical Clustering

- 비슷한 클러스터들끼리 합치는 것을 반복
- 클러스터의 수를 정해주어야 하나, 나중에 정할 수 있음(장점- 역순으로 보면 되니까)
- 데이터가 많아도 잘 작동
- 예전에는 정말 많이 썼는데 요즘엔 보통
- k-means랑 달리 꼭 데이터가 가까이 뭉쳐있어야 할 필요는 없다
- ward, AgglomerativeClustering 그래프 참고 (위 사진)

## DBSCAN

- mean shift랑 비슷한 면이 있음
- 일정 거리(ε)에 최소 N개의 다른 사례가 있으면 핵심점(core point)으
로 지정 → 일정 거리내의 최소 5명의 친구가 있다 (인싸)/ 아니면 아싸
- 거리가 ε보다 가까운 핵심점끼리 연결
- 연결된 핵심점들과 그 핵심점들에서 ε만큼 떨어진 점들로 군집을 구성
- 근데 왜 mean shifit는 안 쓰고 애는 쓰냐? - mean shift는 k-means와 비슷해서 굳이 할 필요가 없지만 bdscan은 다른 양상을 보이고, 큰 데이터에서도 잘 돌아감

## DBSCAN 특징

- 데이터가 많을 때도 잘 작동
- 일정한 형태가 없어도 됨
- 클러스터의 수를 정해주지 않아도 됨
- 단점 : 어느 클러스터에도 속하지 못하는 점들이 생김

### k-mean와 dbscan을 같이 보면 좋음

- 독창적인 값 a가 존재한다고 할 때,
    - k-means는 a 값도 근처 평균 값에 넣어버림
    - 그렇지만 dbscan은 거리 내에 존재하지 않으면 제외해버림
    - 마케팅이라고 생각하면 k-means처럼 구했을 때, a에게 평균과 비슷한 마케팅을 하는 게 효과가 있을까? 라고 할 때 생각해봐야함 (장점 일 수도 아니면 단점 일수도)
    - 그래서 두 개를 같이 보면서 선택하는 것도 좋은 방법

## OPTICS

- DBSCAN의 확장, 일정 거리(ε)를 지정하지 않아도 됨
- 각 사례들을 정렬하고 reachability라는 값을 계산
- reachability가 기준값을 넘어서거나 급증하면 군집을 나눔 (클러스터를 끊음)

![](Untitled-86172a5d-30a8-4631-80c3-215e0a6c1746.png)

## BIRCH

- 대용량 데이터를 빠르게 클러스터링하기 위한 알고리즘
- 트리 구조로 작은 군집(subcluster)들을 먼저 만들고
- 마지막에 작은 군집들을 합쳐 군집화
- 변수가 많을 때는 비효율적

## 클러스터링 평가

- k- means가 가장 좋음
- 사례들이 군집의 중심에서 얼마나 가까운가?
• 낮을 수록 좋음. 0이 최선
• Inertia
• Davies-Bouldin Index

- 다른 군집과 얼마나 다른가?
• 높을 수록 좋음
• Calinski-Harabasz Index
• Silhouette Coefficient

## 클러스터링 평가 지표의 문제점

- 앞에서 소개한 지표들은 k-Means와 같이 각 군집의 주변에 사례들이 볼록(convex)하게 분포하는 경우에만 유효
- DBSCAN 등에서는 적절한 기준이 되지 못함
- **클러스터링의 결과를 바탕으로 행동을 취했을 때 그 성과로 판단**

## 클러스터링과 정답을 비교하는 지표

- Adjusted Rand index
- Mutual Information based scores
- Homogeneity, Completeness and V-measure
- Fowlkes-Mallows scores
- 정답이 있으면 클러스터링을 할 필요가 없으므로 거의 쓸모가 없음

# 복습테스트

1. 다음 각 설명이 선형 회귀분석과 로지스틱 회귀분석 중 어느 쪽에 해당하는지 골라보세요 *

    그래프로 그리면 x와 y의 관계가 직선의 형태가 된다

    그래프로 그리면 s자 형태의 곡선이 된다

    y의 범위가 무한대다

    y가 0에서 1 사이의 범위를 가진다

    y를 확률로 해석한다 → 확률의 범위가 0-1이다

    y가 연속변수일 때 사용한다

    y가 이산변수일 때 사용한다

2. 다음 각 문제를 분석하려고 할 때, "선형 모형"과 "로지스틱 선형 모형" 중 더 적절한 모형을 고르세요. *

    광고비(x)로 제품의 판매량(y)를 예측

    고객 특성(x)으로 재구매 여부(y)를 예측 / 개수 이면 선형 

    식습관(x)으로 환자의 비만도(y)를 예측

    검색어 패턴(x)으로 고객의 성별(y)을 예측

3. 선형모형 y = ax + b에서 계수 a의 신뢰구간은 *

    마이너스와 플러스 양쪽에 고르게 걸쳐 있어야 한다

    마이너스 쪽에만 있어야 한다

    플러스 쪽에만 있어야 한다

    **마이너스든 플러스든 어느 한 쪽에만 있어야 한다**

4. 신뢰수준과 유의수준, 1종 오류와 2종 오류에 대한 설명으로 옳은 것을 모두 고르세요 *

    **신뢰수준이 95%이면 유의수준은 5%이다**

    **유의수준이 5%이면 5%의 경우에는 1종 오류가 발생한다**

    **같은 조건에서 1종 오류을 줄이면 2종 오류가 늘어난다**

    **신뢰수준을 높이면 유의수준이 낮아지고, 유의수준이 낮아지면 1종 오류가 줄어든다. 따라서 2종 오류는 늘어난다**

5. 탐색과 활용 문제에 대한 설명으로 옳은 것을 **모두** 골라 보세요. *

    **탐색은 새로운 시도를 해보는 것**

    **활용은 기존에 알려진 최선의 것을 하는 것**

    **탐색을 많이하면 개선의 기회가 많지만 낭비를 할 수 있다**

    무조건 활용을 많이 하는 것이 좋다

6. 상관계수에 대한 설명으로 올바른 것을 **모두** 골라보세요. *

    **두 변수가 얼마나 선형적 관계가 있는지 나타낸다**

    **-1에서 1까지 범위를 가진다**

    **한 변수가 증가할 때 다른 변수도 증가하면 +가 된다**

    **한 변수가 증가할 때 다른 변수는 감소하면 -가 된다**

    **서열의 상관을 구할 때는 스피어만 상관계수나 켄달 상관계수를 사용한다**

7. 선형 회귀분석을 실시한 결과 R제곱(R-squared)가 0.7이 나왔습니다. 분석 모형은 종속변수의 분산의 몇 %를 설명합니까? *

    10%

    30%

    50%

    **70%**

    90%

8. 다음 중 선형 회귀분석의 모형을 **비교**할 때 사용하는 적합도 지수를 **모두** 고르세요 *

    R-squared

    **Adj. R-squared**

    Log-Likelihood

    **AIC**

    **BIC**

9. 회귀분석에서 독립변수들끼리 서로 예측가능한 경우에 대한 설명으로 올바른 것을 모두 고르세요. *

    **다중공선성이 있다고 한다**

    변수들끼리 상호작용으로 분석한다 → 그냥 두 변수가 종속변수 일 때 어케 서로 영향을 주고 받느냐지, 예측가능하다랑 상관 없음 

    **분석결과가 불안정해진다**

    **다른 변수로 예측 가능한 변수는 제거해야 한다**

10. 변수 선택에 대한 설명으로 올바른 것을 모두 고르세요

    **단계적 회귀분석은 모형이 더이상 개선되지 않을 때까지 단계적으로 모형을 수정하는 방법이다**

    **전방 선택은 변수를 하나씩 추가하는 방법이다**

    **후방 선택은 변수를 하나씩 제외하는 방법이다**

    **교차 검증은 데이터를 두 세트로 나누어 한 세트에서 모형의 파라미터를 추정하고, 다른 세트로 검증하는 방법이다**

    **정규화는 계수의 크기를 작게 만드는 방법이다**

    **정규화를 통해 계수가 0이 되면 변수를 제거하는 것과 사실상 같다**

11. 성별(m)과 운동시간(x)이 근육량(y)에 미치는 영향을 분석하려고 합니다. 이때 다음과 같은 가설을 세웠습니다. (1) 근육량은 운동시간에 따라 달라진다 (2) 같은 시간 운동을 하더라도 성별에 따라 근육량이 늘어나는 정도(기울기)가 다르다 (3) 운동을 전혀 하지 않았을 때 근육량(절편)도 성별에 따라 다르다. 위의 가정을 모형화 했을 때 statsmodels에서 식을 어떻게 입력해야 합니까? *

    y ~ x + m

    **y ~ x * m ==`y~x+m+x:m`**

    y ~ x:m

    y ~ x + x:m

    y~ x + x*m