# Wrap up

- 다시 돌아온 통계 특강

# 1. 확률을 이용한 시뮬레이션

### 변수의 종류

- 연속형, 범주형 (더하기가 되는 게 연속, 안 되는 게 범주형)
- 관찰, 잠재 - 대부분 실무에서 다루는 데이터는 관찰 변수
    - 잠재 변수를 다루는 기법들은 대체로 복잡쓰,,, 그래서 잘 안 다룸 : 믿음 (눈에 안 보이더라도 그런 게 있을 거야) - 고객의 충성도 / 상대방이 나를 얼마나 사랑하는 가는 실제 측정 어렵지만 → 나한테 전화를 얼마나 하는가? 이런 식으로 측정해볼수도?

### 척도

- 비율 (연속)
- 서열 (범주)
- 등간
- 명목

### 대표값

- 최빈값
- 평균
- 중간값
    - 소득 같은 경우는 부자들이 평균 값을 많이 끌어올릴 수 있기 때문에 평균과 중간값 같이 보는 게 좋음

### 산포도

- 평균하고 비슷한 특성이 있어서 극단값이 있으면 범위, 분산, 표준편차, 사분위수가 늘어난다
- 사분위수 : 4토막 낸거 (특징 중 하나 : 비율이 항상 일정)

### 확률 분포

- 정규 분포는 알아야함 (젤 유명)

### 이산확률 분포

- 이항 분포
    - 두 가지 경우가 있을 때 특정 경우가 몇 번 나오느냐 (동전 던지기)
    - 숫자가 작을 때 사용
    - np, np(1-p) 공식
- 다항 분포
    - 여러 가지 경우 존재
    - 각각의 확률이 p (단, 합은 1)
    - n번 했을 때 각 경우가 몇 번 나오는가?
- 기하분포
    - 성공률이 p일 때, 성공할 때까지 시도를 해서 총 시도 횟수
    - ex) 이탈률이 5%인 고객이 가입해서 이탈할 때까지 기간

### 몬테카를로 법

- 확률 분포에 따라 각각의 경우 추출
- 각 경우의 결과 계산
- 경우별 결과를 더하거나 평균 냄

# 변수들 간의 관계 모형

- 조건부확률 (상식처럼 면접에서 물어봤을 때 모르면 안 됨)
    - 어떤 게 주어졌을 때 그 거의 확률 (뒤에 들어가는 게 조건)
    - 공식도 외워두기
    - a와 b의 곱 확률 / a
- 베이즈 정리 외워두기 *데이터 분석가들에겐 애국가 1절, 독도는 우리땅 가사

![](Untitled-1b5d0072-3e00-49a4-a3ca-235280ec745c.png)

- 외웠는 데 아깝다..? 안 물어본다..? 중간중간 딴 거 대답할 때 추임새처럼 넣으세요 ^^
- 베이지언 통계와 빈도주의 통계가 뭐가 다른가 (데.분 단골질문)
    - 보통 우리가 하는 통계는 빈도주의 통계는 다른 말로 `객관주의`
    - 말그대로 '빈도' : 확률을 어떻게 정의하느냐 인데 확률은 빈도 이고 100번 중 7번 벌어졌다 → 0.07 (객관적으로 측정되는 것만 확률이라고 함)
    - 극단적으로 말하면 2019년에 한국 야구에서 sk가 우승할 확률? 이런건 없음 왜냐, 2019년이 100번 존재하는 건 아니니까.
    - 어차피 n이 하나니까 우승하면 1이고 아니면 0.
    - 베이지언 통계는 `주관주의` , 통계에서 확률을 보는 관점이 다름 : 믿음의 정도
    - ex) 2019년 프로 야구에서 sk가 우승할 확률 → 내가 얼마나 sk가 우승할건지를 믿는 정도가 확률이라고 봄 (그래도 데이터를 가지고 믿음 - 유리한 증거가 나타나면 더 믿는 거고, 불리한 증거가 나타나면 덜 믿는 거고.) 이런 식으로 믿음의 정도를 조정해 감
    - 베이즈언 통계는 통계하는 사람들의 마음의 고향 같은 느낌,, 그치만 계산하기 어려움. 분석할 때 잘 안 씀.
    - 정의를 외워두는 게 좋음
        - 가끔씩 나올 때 → 텍스트 주제 분석 : latent dirichlet allocation / 시계열 분석 구글에서 만든 causal impact
        - 책 추천 : the book of why
- 우도 : 중요하니까 외우기
    - 모형이 주어짐
    - 모형 m을 가정했을 때, 데이터 d가 나올 확률
    - 대부분 우도만 구함
- 사후확률
    - 데이터가 주어짐
    - 데이터 d를 관찰했을 때 모형 m이 참일 확률
- 그래프 모형 부분은 사실 잘 모르셔도 됩니다,,,,그냥 이론 설명

### 독립변수와 종속변수

- 뭐가 이산/연속 변수이냐에 따라 다르다.
- x쪽보단(독립변수) 보단 종속변수(y)가 더 중요

### 잠재변수

- 클러스터링 : 독립변수가 이산변수여서 x가 카테고리컬(범주형)이여서 우리가 모르는 거. (노트 필기 참고)

![](Untitled-81a4e197-3918-4392-acfd-d4d2970ac11c.png)

# 결측값 채우기

- 데이터가 없는 데 이유가 있는 경우/ 없는 경우
- 데이터가 무작위로(random) 지워져 있다면
- listwise deletion:
    - 결측값이 있는 사례는 삭제
    - 결측값 있는 사례가 매우 소수일 때
    - 제한적인 경우에만 쓰일 수 있고, 비싼 데이터는 아까우니까 안함(의료 데이터)
- imputation : 결측값을 추측해서 채워넣는 것
    - 회귀 분석 (패키지가 많이 존재) : python imputation
    - 비어있는 컬럼을 다른 컬럼으로 회귀분석 등을 해서 예측하여 채워넣는 것
    - 예측값 하나만 채워넣으면 분산이 작아지는 문제가 생김
    - multiple imputation: 예측값을 여러 개로 채워넣어 분산까지 복원 → 데이터가 부풀려지는 문제
    - 여러 방식을 시도해서 가장 좋은 데이터로 하기
- 모형 자체를 결측값이 있어도 상관없게 만드는 것 (어려워서 잘 안 씀)

- imbalanced data
    - 화재/비화재 : 대부분의 경우에는 비화재
    - 뭐가 중요한 지 결정하기 (화재를 맞추는 게? 아니면 비화재? 아니면 맞추기만 하면 된다?!)
    - 지표 : precision, recall, accuracy
- imbalanced 전략
    - undersampling : 많은 쪽 (majority) 데이터를 없앤다.
        - 비화재 데이터를 못 맞추는 이유는 너무 많으니까, 그걸 없앤다 (화재 데이터 수 만큼)
        - 데이터가 줄어드는 단점
    - overdampling: 적은 쪽 (minority) 데이터를 늘린다.
        - 화재 데이터가 100건이라면 비화재 데이터에 맞춰서 200건 정도로 늘린다. (중복으로 넣는 방법은 쉽고 무식함) → 약간씩 노이즈를 썩어도 되고, 많이 쓰는 방법은 smote(데이터 합성 : 데이터 2개의 평균으로 새로운 데이터를 만듬)
        - 데이터가 인위적으로 불리져기 떄문에 데이터를 왜곡함 - 알고리즘에 따라서 불리는 방법이 다름
        - 다 해보고 지표에 맞는 기법 선택
        - smote : imbalaced_learnd 패키지가 유명
        - [https://imbalanced-learn.readthedocs.io/en/stable/](https://imbalanced-learn.readthedocs.io/en/stable/)

    ### 시계열 분석

    - 시계열: 이전 시점의 일이 이후 시점에 영향
    - 그냥 회귀
        - 주차대수 = 요일 + 날씨 + 계절 + 공휴일
    - 그냥 분류(예: 로지스틱)
        - 주차가능 ? = 요일 + 날씨 + 계절 + 공휴잉ㄹ
    - 시계열
        - 오늘의 주차대수 = 어제의 주차대수 + 요일 + 날씨 + ...

    ### 뻔히 알고 있는 걸 왜 분석하나요?

    - 그게 아닐 수도 있음 (사후약방문)
        - hindsight bias(후견지명)
    - 실제로 아는 경우

    ### 동전 던지기

    - 앞면이 나올 확률이 60%인 동전을 10개 던졌을 때
        - r제곱에서 나오는 분산이 무슨 상관?
        - r 제곱 : mse를 분산으로 나누어서 1 뺀 거 (1-mse/var)
        - mse =(실제-예측)~~의 제곱의 평균~~
        - var 분산 : (실제-평균값)~~의 제곱의 평균~~
        - mmm = (실제 - 중간값)~~의 제곱의 평균 : 잘 안하는 방식~~
        - mae =  | 실제값 - 예측값 |의 평균 → mmm의 절대값과 비교
        - r 제곱에서 mse와 분산, 둘의 비율이 어떻게 되느냐 가 중요
        - r 제곱 = 0 → mse = var
        - 분산을 작은 값으로 보는 이유? → 분산도 오차로 볼 수 있으니까, 작은 값 일 수록 오차가 작아지니까,,,
        - 비교 기준을 정할 땐 여러가지 방법이 있지만 최적의 방법을 도출하기 위해 노력해야 함
    - 이항분포

    # 클러스터링

    - 분류와 클러스터링의 차이
    - k - means에서 k를 어떤 방식으로 정해야하는가?
        - 1) 데이터의 분산을 보고 k를 정하는 방법도 있고
        - 2) k 숫자를 늘려가면서 어떤 때 평균에 가장 몰려있을 지를 살피면서 하면 됨
    - 클러스터링 지표 평가
        - k-means 는 평균이 있기 때문에 성능을 지표하기가 좋음

    ![](Untitled-58ecb4c6-ae06-4db0-899e-4706a76cfc56.png)

    ![](Untitled-fd5e8401-ede2-4589-b4ee-262440c7c332.png)