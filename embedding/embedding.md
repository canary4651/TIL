# 특강/임베딩_자연어처리

### 이기창님
- ratsgo@naver.com

# 임베딩?

- 자연어처리 핵심
- 단어나 문장을 벡터로 바꾼 것 혹은 그 과정 : 컴퓨터는 계산기일 뿐, 사람의 말은 이해를 못한다
- 가장 쉽게 바꾸는 법 : 빈도
- 문서 임베딩/ 단어 임베딩
    - 사람은 직관적으로 이해할 수 있지만, 컴퓨터는 이해 못함

![](Untitled-ded83b30-a586-4ab3-b195-ef2dcec44c85.png)

- 희망이라는 단어의 word2vec 임베딩
    - 실제 현업에서 쓰이는 임베딩 기법 (word → vector)
    - 총 100차원
    - 희망이라는 단어를 바꾸는 과정도 임베딩, 나온 결과도 임베딩

![](Untitled-bbbad09c-5a55-420d-9eeb-17f7342452fc.png)

## 임베딩으로 할 수 있는 것

- 관련도/유사도 계산

![](Untitled-f6006a79-d51b-40ba-87b2-4d45baa8ec0b.png)

![](Untitled-721803fc-6807-403e-ab27-c5d5d05a511c.png)

![](Untitled-4dad9430-9e59-404a-a278-4d862d82abf9.png)

- 벡터 연산(유추 평가) : 아들 - 딸 + 소녀 = 소년
    - 아들 - 딸 = 소년 - 소녀

![](Untitled-0ccd97b4-78d9-4632-a0f5-f54e3f2d14bd.png)

![](Untitled-5a4d12a0-2862-41fc-967f-ac8637007e76.png)

- 전이학습 : 다른 딥러닝 모델의 입력값(transfer learning)

![](Untitled-fd9141f8-9100-4e6b-8d44-3b425d18c644.png)

![](Untitled-8d1c19e1-e7da-49bb-afeb-bc09f97b1e82.png)

![](Untitled-d4aa131a-aff6-4b63-a752-e5f46edefe88.png)

- 정확도는 높고

![](Untitled-072b3989-24cf-4a62-9a91-de1b5e693ff9.png)

- 수렴은 빠르다 : 오차가 처음부터 작고 (수렴: 오차값을 원하는 수준까지 떨어뜨리는 것) 빨라짐 (random으로 안 돌리고 word2vec이나 fasttext로 미리 학습 시키면!)

## 단어 임베딩으로 문서 분류하기(1)

- 핵심 컨셉 : 문서에 속한 단어가 유사하면 문서 의미도 비슷하다
- 문서 벡터를 어떻게 만드나? : 단어 임베딩의 합으로 문서 벡터를 표현
- 실험으로 검증 : 학습 5만/테스트 1만건, binary classification

![](Untitled-43ebf844-2b6d-4a94-9581-a52c15b42aad.png)

- 시사점
    - 복잡한 딥러닝 모델을 써도 80%대 성능 기록
    - **단어 임베딩 품질이 좋으면 자연어 처리 성능을 높일 수 있음**

## 단어 임베딩으로 문서 분류하기(2)

- 핵심 컨셉 : 유의어가 많이 포함된 문장을 자동으로 추출한다
- 임베딩이 어떻게 의미를 가지는가
- **말뭉치의 통계적 패턴 정보가 들어 있다**
    1. **빈도**를 센다 : 문서를 쓴 이의 의도는 단어 사용 패턴에 드러난다 
        - Bag of words 가정
    2. 단어가 어떤 **순서**로 나타나는지 살핀다 : 시퀀스 정보에 의미가 녹아 있다 
        - ELMo(한방향), BERT(양방향)→ top 찍고 있음 : 쓸 때 얼마나 효율 좋게 줄이느냐, 계산양을 어떻게 빠르게 서빙하느냐가 요즘 경쟁력
        - Language Model : 확률 모델 (단어 시퀀스에 확률을 부여)
            - f(w1,w2,.......,wn) = prob. → 시퀀스가 그럴 듯하면 prob가 높을 것.
            - 잘 학습된 언어 시퀀스는 문장을 생성할 수 있음

        ![](Untitled-63e79afe-78d3-4592-a827-df77c2f293c8.png)

    3. 단어가 어떤 단어와 주로 **같이** 나타나는 지 살핀다 : 문맥에 의미가 녹아 있다

## 문장 수준 임베딩 구축 및 활용

- 문장 수준 임베딩 : 최근 ELMo, BERT 등 다양한 문장 수준 임베딩 등장
- 문장 수준 임베딩의 장점은 동음이의어 분간 가능, 다시 말해 문장의 문맥적 의미를 벡터화할 수 있음
    - (배가 고파, 배가 항구에 정박했다, 배가 달다)
- 임베딩이 가장 크게 쓰일 수 있는 분야는 전이학습. 다른 네트워크의 입력값으로 사용돼 자연어 처리 성능을 높일 수 있음

# 친근하고 유익한 대화 챗봇 만들기

- 위로가 되는 리액션, 친근함
- 모델 기반 챗봇
- 모델이 답변을 찾는 방법 1
    - 질의를 바탕으로 답변을 새롭게 생성한다
    - 문제점 : 답변 내용과 품질에 개입할 수 없다 : 문법상 비문을 컨트롤 못함 , 가짜 뉴스 , 특정 집단 비하,
- 답변을 찾는 방법 1
    - 사용자 질의와 가상 유사한 q를 미리 준비한 데이터에서 선택. 해당 q에 연결된 a를 답변으로 내보낸다
    - 답변 커버리지가 떨어지지만 답변 내용을 컨트롤 할 수 있음

- 모델의 영역보단 기획에 가까움. 정해진 답변 셋 안에서 모든 대화를 커버할 수 있어야 함. (대화 기획)
- 따라서 중요한 것은 **대화 경험 설계**

### 페르소나

- 긍정적이지만 가볍지는 않다
- 성별, 나이, 외모 등을 확정 지어서 답변하지 않는다
- 호칭, 존대 여부, 성별, 연령대 등 일관성을 가져야 함
- 답변 할 때 일관성 있이 말하는 게 매우 중요하다

# task-oriented dialog system

- 요즘 뜨는 챗봇은 심리적 안정감, 위로감이 아니라 태스크를 수행할 수 있는 챗봇이 돈이 됨
- 콜 센터를 대처하길 원함 (예약, 구매, 배송 등)
- 네이버 클로바 contact center
- 전화망 기본 챗봇  : 끊김도 겹침도 없어야 함
- task 수행엔 다양한 시나리오 존재
- 중요한 것은 대화 경험 설계